# ğŸ“Š Sentiment Classifier â€” DistilBERT + FastAPI + Docker + UI

This project demonstrates a full AI workflow: training a DistilBERT model on IMDB movie reviews, serving it via a FastAPI backend, containerizing it with Docker, and testing it with a simple UI.

---

## âœ… Features

* Fine-tunes DistilBERT on IMDB for binary sentiment classification (positive/negative)
* Tokenizes reviews using Hugging Face Tokenizer
* Trains model using Hugging Face Trainer
* Saves model (.pt) and tokenizer locally
* Serves predictions via FastAPI
* Dockerized for production-ready deployment
* Includes simple browser-based UI to test the API
 
---

## ğŸ§  Tech Stack

* ğŸ¤— Hugging Face Transformers + Datasets
* ğŸ§  DistilBERT (pretrained model)
* ğŸ”¥ PyTorch
* âš¡ FastAPI
* ğŸ³ Docker
* ğŸŒ HTML + JavaScript (for UI)

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ tokenizer/              # Saved tokenizer files
â”œâ”€â”€ main.py                 # FastAPI API for inference
â”œâ”€â”€ train_and_save.py       # Script to train and save model/tokenizer
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ index.html              # Simple browser UI
â””â”€â”€ README.md
```

---

## ğŸ How to Run (Locally)

### ğŸ”¹ 1. Train the model

```bash
python train_and_save.py
```

* Downloads IMDB dataset
* Fine-tunes DistilBERT for 1 epoch on 10k samples
* Saves `sentiment_model.pt` and tokenizer

### ğŸ”¹ 2. Serve with FastAPI

```bash
uvicorn main:app --reload
```

* FastAPI server runs on `http://localhost:8000`
* Test at `http://localhost:8000/docs`

### ğŸ”¹ 3. Test with UI

Open `index.html` in a browser and enter a movie review.

---

## ğŸ³ Docker Instructions

### ğŸ”¹ Build Docker Image

```bash
docker build -t sentiment-classifier .
```

### ğŸ”¹ Run Container

```bash
docker run -p 8000:8000 sentiment-classifier
```

### ğŸ”¹ Access FastAPI

Go to `http://localhost:8000/docs` to test Swagger UI.

---

## ğŸ” Notes

* `256` max token length used instead of 512 â€” covers \~85% of reviews efficiently
* Model trained on subset (10k train / 1k eval) for fast prototyping
* Full training on 25k samples improves accuracy
* Tokenizer is saved locally to ensure identical preprocessing at inference

---

## ğŸ¯ Prediction Output

Returns `{ "prediction": 1 }` â†’ where 1 = Positive, 0 = Negative

---
